**GFS论文笔记**

> //https://www.cnblogs.com/guoyongrong/p/3700970.html
>
> **简介**
>
> 首先，节点失效将被看成是正常情况，而不再视为异常情况。其次，按照传统标准来看，文件都是非常巨大的。数个GB的文件是常事。每一个文件都包含了很多应用程序对象。第三，大部分文件都是只会在文件尾新增加数据，而少见修改已有数据的。对一个文件的随机写操作在实际上几乎是不存在的。第四，与应用一起设计的的文件系统API对于增加整个系统的弹性适用性有很大的好处。
>
> **接口**
>
> 文件是通过pathname来通过目录进行分层管理的。
>
>  **架构**
>
> GFS集群由一个单个的master和好多个chunkserver（块服务器）组成，GFS集群会有很多客户端client访问。每一个节点都是一个普通的Linux计算机，运行的是一个用户级别（user-level）的服务器进程。在GFS下，每一个文件都拆成固定大小的chunk(块)。每一个块都由master根据块创建的时间产生一个全局唯一的以后不会改变的64位的chunk handle标志。chunkservers在本地磁盘上用Linux文件系统保存这些块，并且根据chunk handle和字节区间，通过LInux文件系统读写这些块的数据。出于可靠性的考虑，每一个块都会在不同的chunkserver上保存备份。缺省情况下，我们保存3个备份，不过用户对于不同的文件namespace区域，指定不同的复制级别。
>
> master负责管理所有的文件系统的元数据。包括namespace，访问控制信息，文件到chunk的映射关系，当前chunk的位置等等信息。master也同样控制系统级别的活动，比如chunk的分配管理，孤点chunk的垃圾回收机制，chunkserver之间的chunk镜像管理。master和这些chunkserver之间会有定期的心跳线进行通讯，并且心跳线传递信息和chunckserver的状态。
>
> ![](../golang%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/golang%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/.assets/GFS%E6%9E%B6%E6%9E%84.PNG)
>
> **master**
>
> 必须尽量减少master的读和写操作，以避免它成为瓶颈。客户端永远不会通过master来做文件的数据读写。客户端只是问master它应当访问那一个chunkserver来访问数据。客户端在一定时间内cache这个信息，并且在后续的操作中都直接和chunkserver进行操作。
>
> 首先，客户端把应用要读取的文件名和偏移量，根据固定的chunk大小，转换成为文件的chunk index。然后向master发送这个包含了文件名和chunkindex的请求。master返回相关的chunk handle以及对应的位置。客户端cache这些信息，把文件名和chunkindex作为cache的关键索引字。于是这个客户端就像对应的位置的chunkserver发起请求，通常这个会是离这个客户端最近的一个。请求给定了chunk handle以及一个在这个chunk内需要读取得字节区间。在这个chunk内，再次操作数据将不用再通过客户端-master的交互，除非这个客户端本身的cache信息过期了，或者这个文件重新打开了。实际上，客户端通常都会在请求中附加向master询问多个chunk的信息，master于是接着会立刻给这个客户端回应这些chunk的信息。这个附加信息是通过几个几乎没有任何代价的客户端-master的交互完成的。
>
> **chunk的大小**
>
> 我们选择这个大小为64M。每一个chunk的实例（复制品）都是作为在chunkserver上的Linux文件格式存放的，并且只有当需要的情况下才会增长。滞后分配空间的机制可以通过文件内部分段来避免空间浪费。
>
> 选择一个很大的chunk大小提供了一些重要的好处。首先，它减少了客户端和master的交互，因为在同一个chunk内的读写操作只需要客户端初始询问一次master关于chunk位置信息就可以了。这个减少访问量对于我们的系统来说是很显著的，因为我们的应用大部分是顺序读写超大文件的。即使是对小范围的随机读，客户端可以很容易cache一个好几个TB数据文件的所有的位置信息。其次，由于是使用一个大的chunk，客户端可以在一个chunk上完成更多的操作，它可以通过维持一个到chunkserver的TCP长连接来减少网络管理量。第三，它减少了元数据在master上的大小。这个使得我们可以把元数据保存在内存。面对小型文件不太友好！！！在以下案例下， 一个可执行的程序在GFS上保存成为一个单chunk的文件，并且在数百台机器上一起启动的时候就出现焦点问题。只有两三个chunkserver保存这个可执行的文件，但是有好几百台机器一起请求加载这个文件导致系统局部过载。我们通过把这样的执行文件保存份数增加，以及错开batchqueue系统的各worker启动时间来解决这样的问题。一劳永逸的解决方法是让客户端能够互相读取数据，这样才是解决之道。
>
> master节点保存这样三个主要类型的数据：文件和chunk的namespace，文件到chunks的映射关系，每一个chunk的副本的位置。所有的元数据都是保存在master的内存里的。头两个类型（namepspaces和文件到chunk的映射）同时也是由在master本地硬盘的记录所有变化信息的operation log来持久化保存的，这个记录也会在远端机器上保存副本。通过log，在master宕机的时候，我们可以简单，可靠的恢复master的状态。master并不持久化保存chunk位置信息。相反，他在启动地时候以及chunkserver加入集群的时候，向每一个chunkserver询问他的chunk信息。
>
> **内存数据结构**
>
> 定时内部状态扫描是用于实现chunk的垃圾回收机制，当chunkserver失效的时候重新复制，以及为了负载均衡和磁盘空间均衡使用的目的做chunkserver之间的chunk镜像。这种内存保存数据的方式有一个潜在的问题，就是说整个系统的chunk数量以及对应的系统容量是受到master机器的内存限制的。master为每64Mchunk分配的空间不到64个字节的元数据。大部分的chunks都是装满了的，因为大部分文件都是很大的，包含很多个chunk，只有文件的最后部分可能是有空间的。类似的，文件的名字空间通常对于每一个文件来说要求少于64个字节，因为保存文件名的时候是使用前缀压缩的机制。
>
> **chunk的位置**
>
> master并不持久化保存chunkserver上保存的chunk的记录。它只是在启动的时候简单的从chunkserver取得这些信息。master可以在启动之后一直保持自己的这些信息是最新的，因为它控制所有的chunk的位置，并且使用普通心跳信息监视chunkserver的状态。因为这样可以消除master和chunkserver之间进行chunk信息的同步问题，当chunkserver加入和离开集群，更改名字，失效，重新启动等等时候，如果master上要求保存chunk信息，那么就会存在信息同步的问题。在一个数百台机器的组成的集群中，这样的发生chunserver的变动实在是太平常了。
>
> 此外，不在master上保存chunk位置信息的一个重要原因是因为只有chunkserver对于chunk到底在不在自己机器上有着最后的话语权。另外，在master上保存这个信息也是没有必要的，因为有很多原因可以导致chunserver可能忽然就丢失了这个chunk（比如磁盘坏掉了等等），或者chunkserver忽然改了名字，那么master上保存这个资料啥用处也没有。
>
> **操作记录**（临界区资源）
>
> 操作日志包含了关键的元数据变化的历史记录，是GFS的核心。它不仅永久的记录了元数据，还能提供确定并发操作顺序的逻辑时间线服务。文件和块，连同它们的版本，都是由它们创建的逻辑时间唯一的、永久的进行标识的。
>
> 因为操作日志是临界资源，我们必须可靠的存储它，在元数据的变化进行持久化之前，客户端是无法看到这些操作日志的。否则，即使块本身保存下来，仍然有可能丢失整个文件系统或者客户端最近的操作。因此，我们将它复制到几个远程的机器上，并在将相应的操作刷新（flush）到本地和远程磁盘后回复客户端。主节点会在刷新之前批处理一些日志记录，因此减少刷新和系统内复制对整个系统吞吐量的影响。
>
> 主节点通过重新执行操作日志来恢复状态。为了使启动时间尽量短，我们必须保持日志较小。当日志超过一个特定的大小时，主节点会检查它的状态，以使它能够通过载入本地磁盘的最后一个检查点，并重新执行检查点（checkpoint）后的日志记录进行恢复。检查点是一个压缩B树类似的形式存储，能够直接映射到内存中，并且在用于命名空间查询时无需额外的解析。这大大提高了恢复速度，增加了可用性。
>
> 因为创建检查点需要一定的时间，所以主节点的内部状态会结构化为一种格式，在这种格式下，新检查点的创建不会阻塞正在进行的修改操作。主节点切换到新的日志文件，并通过另一个线程进行新检查点的创建。新检查点包括切换前所有的修改操作。对于一个有几百万文件的集群来说，创建一个新检查点大概需要1分钟。当创建完成后，它将写入本地和远程磁盘。
>
> 恢复只需要最近完成的检查点和在此之后的日志文件。老的检查点和日志文件能够被删除，但为了应对灾难性故障，我们会保留其中的一部分。检查点的失败不会影响恢复的正确性，因为恢复代码会探测并跳过未完成的检查点。
>
> **一致性模型**
>
> **GFS的可靠性保证**
>
> 文件名字空间的改变（比如，文件的创建）是原子操作。他们是由master来专门处理的。名字空间的锁保证了操作的原子性以及正确性（4.1节）；master的操作日志定义了这些操作的全局顺序
>
> ![](../golang%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/golang%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/.assets/GFS%E6%96%87%E4%BB%B6%E4%B8%80%E8%87%B4%E6%80%A7.PNG)
>
> 一个文件区域（region）在一个数据修改后的状态依赖于该修改的类型、成功或失败、以及是否为同步的修改。如果所有的客户端无论从哪些副本读取数据，得到的数据都是相同的，则这个文件区域为一致的。在一个文件数据修改后，如果它是一致的，则这个区域已定义，客户端将看到被写入的全部内容。当一个修改没有受到并发写入操作的影响而成功时，那么影响到的区域已定义（隐含一致性）：所有的客户端将总会看到修改已经写入的内容。并发成功的修改使区域处于未定义，但一致的状态：所有的客户端将看到相同的数据，但是它可能不能反映出任意一个修改已写入的内容，通常，它是由一些来自多个修改操作的、混合的数据碎片组成的。一个失败的修改会造成区域的不一致性（因此也没有被定义）：不同的客户端在不同的时间可能会看到不同的数据。
>
> 一个写操作会将数据写入到应用指定的文件位置；即使有多个并发修改操作，一个记录追加操作会将数据（记录）至少一次的原子性的追加到文件，而具体的偏移量是由GFS选定的。偏移位置返回给客户端，并标明包含这个记录的已定义区间的开始位置。此外，GFS能够在文件中插入数据或复制记录，这些数据占据的文件区域被认为为非一致性的，数量通常比用户数据总量要小很多。
>
> 在一系列成功的修改操作后，被修改的文件区域被保证为是已定义的，并且包含了最后一次修改操作写入的数据。GFS通过以下措施完成上面行为：（a）在所有副本上按相同的顺序执行一个块上的修改操作（3.1），（b）使用版本号来检测并复制过期文件（4.5），这种过期可能是由于块服务器宕机而造成了部分修改丢失引起的。过期的副本不会再涉及修改操作，主节点也不会将该副本返回给客户端。它们会尽快的进行垃圾回收操作。
>
> 因为客户端缓存了块位置信息，它们可能会在位置信息更新前直接访问过期的副本。这个窗口是受缓存条目的超时时间和下一次文件的打开时间限制的，下一次文件打开才会对这个文件的所有块信息的缓存进行清理。此外，由于大多数文件都是只能追加的，一个过期的副本经常会返回一个过早结束的块而不是过期的数据。当一个读客户端再次与主节点查询时，它将会立即获得当前块的位置信息。
>
> 在一个修改操作成功执行完一段时间后，组件的失效依然能损坏或删除数据。GFS通过在主节点和所有块服务器间定期的握手来标识失效的块服务器，并通过校验和探测数据损坏（5.2）。一旦问题被发现，数据将会尽快的利用有效副本进行恢复（4.3）。只有在GFS反应之前，通常在几分钟之内，一个块的所有副本都丢失，这个块才会彻底的丢失。即使在这种情况下，它会变为不可用，而不是被损坏的数据：应用会接收到明确的错误码而不是损坏的数据。
>
> **GFS实现**
>
> 一个宽松的一致性模型：依赖追加而不是覆盖、检查点、以及写入自验证、自标识的记录。检查点也含有应用层的校验和。读操作仅验证并处理上一个检查点之后的文件区域，这个文件区域应该是已定义的。追加写操作比随机写操作更有效率，对应用的错误更有弹性。检查点允许写操作逐步的进行重启，并防止读操作处理已成功写入但在应用角度没有完成的文件数据。每个写操作写入的记录都包含了一些额外的信息，如校验和，以使这个记录能够被验证。一个读操作能够通过校验和识别并丢弃额外的填充数据和记录碎片。如果它不能处理偶而的重复数据（如，如果他们将引发非幂等的操作），它能通过记录中的唯一标识来进行过滤，这些标识通常用于命名相关的应用实体，如网页文档。这些记录I/O的函数（除了删除重复数据）都在应用的共享库中，并且适用于Google其它的文件接口实现。于是，相同序列的记录，加上偶尔出现的重复数据，总是被分发到记录的读操作上。
>
> **租约（lease）和修改操作顺序**
>
> 我们使用租约来保证一个一致性操作在副本间的顺序。Master将一个块租约授予所有副本中的一个，这个副本成为primary。Primary对一个块的所有修改操作选择一个串行顺序，所有的副本在执行修改时都按照这个顺序。因此，全局的修改操作的顺序首先由Master选择的授予租约的顺序决定，然后由租约中primary分配的序列号决定。
>
> 租约机制的设计是为了最小化Master的管理开销，一个租约初始超时时间为60秒，然而，只要块正在被修改，primary就能请求续约，并通常会收到主节点租约延长的响应。这些续约请求和授予都包含在主节点和所有块服务器间定期交换的心跳消息中。主节点有时会在租约到期之前尝试取消它（如，当Master想要禁止对一个被重命名的文件的修改操作时）。即使Master与primary失去通信，它也能在前一个租约到期后安全的将一个新租约授予另一个副本。
>
> ![](../golang%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/golang%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/.assets/%E5%86%99%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B.PNG)
>
> 我们通过跟踪一个写操作的控制流的这些步骤，描述了这个过程：
>
> 1. 客户端询问Master哪个块服务器持有这个块的当前租约，以及这个块的其它副本位置。如果没有一个租约，则Master选择一个副本并授予一个租约（没有在图上显示）。
> 2. Master回复客户端primary的标识，以及其它（secondary）副本的位置。客户端缓存这些数据用于以后的修改操作。只有当primary不可达或者接收到primary不再持有租约时才需要再一次请求主节点。
> 3. 客户端将数据推送到所有的副本。一个客户端能够以任意顺序进行推送。每个块服务器将数据保存在内部的LRU缓存中，直到数据被使用或者过期被替换掉。通过对数据流和控制流的分流，我们能够通过基于网络拓扑来调度数据流，不管哪个块服务器为primary，以此提高性能。3.2节将进一步讨论。
> 4. 一旦所有的副本都确认接收到了数据，客户端将向primary发送一个写请求。这个请求确定了之前的数据被推送到了所有副本。Primary为接收到的所有修改操作分配连续的序列号，这些操作可能来自多个客户端，序列号提供了严格的序列化，应用按序列号顺序执行修改操作，进而改变自己的状态。
> 5. Primary将写请求发送到所有的secondary副本上。每个secondary副本按照primary分配的相同的序列号顺序执行这些修改操作。
> 6. Secondary副本回复primary，表示它们已经完成了所有的操作。
> 7. Primary回复客户端。任意副本上的任意错误都将报告给客户端。在一些错误情况下，写操作可能在primary和一些secondary副本上执行成功。（如果失败发生在primary，它将不会分片一个序列号，并且不会被传递。）客户端的请求被视为已经失败，这个修改的区域停留在不一致的状态上。我们的客户端代码通过重试失败的修改操作来处理这种错误。在从头开始重复执行之前，它将在3-7步骤上做几次尝试。 
>
> 备注：（由于写入的位置时由GFS指定的， 即所有副本写入位置是确定且一致的， 这样对正常完成写操作的副本并不会影响。）
>
> 如果一个应用的写操作数据很大或者跨越了一个块的边界，GFS客户端会将这个操作分割成几个写操作。但是可能会被其它客户端上的请求打断或覆盖。因此，共享的文件区域可能最终包含不同客户端的数据碎片，尽管如此，所有的副本都是完全相同的，因为这些独立的写操作在所有副本上都按着相同的顺序成功执行。
>
> **数据流**
>
> 控制流从客户端到primary上，再到所有的secondary上，而数据则以管道形式、线性的沿一个精心设计的块服务器链进行推送。为了有效利用每台机器的网络带宽，数据被线性的沿一个块服务器组成的链进行推送，而不是按着其它拓扑进行分发（如，树）。因此每台机器的出口带宽都用于以最快的速度传输数据，而不是分配给多个接收者。为了尽可能的避免网络瓶颈和高延迟链路，每台机器只将数据推送给网络拓扑中最近的没有接收到数据的机器。最后，我们通过在TCP连接上使用管道传输数据来最小化时间延迟。
>
> **原子性的记录追加**
>
> GFS提供了一种原子性的追加操作称为记录追加。在一个记录追加操作中，客户端只需要指定数据，GFS将数据至少一次的原子性的追加到文件（如，一个连续的字节序列）的GFS选定的一个偏移位置，并将这个偏移位置返回给客户端。客户端把数据推送到文件最后一个块的所有的副本上，然后将向primary发送它的请求。Primary会检查这次追加操作是否使块的大小超过了最大尺寸（64MB）。如果超过，它将把这个块填充满，通知所有的secondary副本进行相同的操作，并回复客户端表明这个操作将在下一个块上重新执行。（记录追加操作的数据大小严格控制在最大尺寸的1/4以内，以确保最坏情况下碎片的数量在一个可接受范围。）通常情况下，如果记录不超过最大尺寸，primary将数据追加到它的副本上，然后通知secondary把数据写到与primary相同的位置上，最后回复客户端操作成功。
>
> 如果在任意一个副本上的记录追加失败，客户端将重试这个操作。因此，在同一个块的副本上可能包含不同的数据，包括同一个记录的全部或部分的重复数据。GFS不保证写入的数据在字节上完全相同，它只保证作为一个原子单元至少被写入一次。这个特性能够通过简单的观察得到：如果操作执行成功，数据肯定被写入到了某些块副本的相同位置。此外，在这之后，所有副本至少都达到了记录尾部的长度，因此，即使一个不同的副本成为了primary，以后的任何记录也都将被放置在更大的偏移位置或者是一个不同的块上。在我们的一致性保障方面，记录追加操作成功的写入数据的区域是被定义的（因此是一致的），反之，介于中间状态的区域是不一致的（因此是未定义的）。 
>
> **快照**
>
> 快照操作几乎瞬间的为一个文件或一个目录树（源）创建一个拷贝，并且不会对正在进行的其它操作造成任何影响。我们的用户使用它为一个巨大的数据集创建一个拷贝分支（而且经常递归的对拷贝进行拷贝），或者是在尝试变化之前对当前的状态创建检查点，之后可以轻松的进行提交或回滚。
>
> 我们使用标准的写时拷贝（Copy-On-Write）技术来实现快照。当Master接收到一个快照请求时，它先取消快照相关的文件块的所有租约。这确保了任何后面对这些块的写操作将需要与Master进行交互，以获取租约的持有者，这将为Master提供一个为块创建一个新拷贝的机会。 
>
> 在租约被取消或者过期后，Master将这些操作记录到磁盘，然后通过复制源文件或目录树的元数据在它内存中的状态上执行这些日志记录。新创建的快照文件与源文件指向相同的块。
>
> 在快照操作后，客户端第一次想要向块C中写入数据前，它将向Master发送一个请求来查找当前的租约持有者。Master注意到块C的引用计数大于1，它将推迟回复客户端的请求，并选择一个新的块句柄C’，然后通知每个拥有块C副本的块服务器，创建一个新的块C’。通过在同一个块服务器上创建一个新的块，我们能确保这个拷贝是本地的，不需要通过网络进行的（我们的磁盘速度是100MB以太网链路的3倍）。从这点上看，请求的处理不会与其它的块处理有差别：主节点将新块C’的租约授予其中一个副本，并回复客户端，客户端能够进行一般的写操作，并不知道这个块是从一个已存在的块上创建出来的。
>
> **Master操作**
>
> 它管理整个系统内的所有块的副本：它决定块的存储位置，以及协调系统范围内的各种行为以保障块能够有足够的副本，均衡所有块服务器的负载，以及回收不再使用的存储空间。
>
> GFS没有一个目录数据结构，它列出了这个目录下的所有文件，也不支持对文件和目录的别名操作（如，Unix术语中的硬链接或符号链接）。GFS的命名空间逻辑上表现为一个将全路径名映射为元数据的查询表。利用前缀压缩，这个表能够高效的存储在内存中。每个命名空间树中的节点（无论是一个绝对文件名还是一个绝对目录名）都有一个与之关联的读写锁。
>
> 每个主节点操作在执行前都先获得一系列的锁，通常，如果它涉及到/d1/d2/.../dn/leaf，它将获得目录名的读锁/d1，/d1/d2，...，/d1/d2/.../dn，然后获取完全文件名/d1/d2/.../dn/leaf的一个读锁或写锁。注意，这里的leaf根据其操作，可能是一个文件，也可能是一个目录。{
>
> 我们现在能阐明在/home/user快照成/save/user时，这个锁机制如何防止创建一个文件/home/user/foo。快照操作获得了/home和/save上的读锁，以及/home/user和/save/user上的写锁，而文件创建操作则获得了/home和/home/user上的读锁，以及/home/user/foo上的写锁。因为它们都试图获得/home/user上的锁而造成冲突，所以这两个操作将适当的进行排序。文件创建不需要获取它的父目录的写锁，因为这里没有“目录”或类似inode等用来防止被修改的数据结构。文件名上的读锁足以防止父目录被删除。}
>
> 所以读写锁对象采用惰性分配策略，一旦不再使用则被删除。同样，锁需要按一个相同的顺序被获取来防止死锁：他们先对命名空间进行排序，并在同一级别按字典序排序。
>
> **副本布局**
>
> 最大化数据的可靠性和可用性，最大化带宽利用率。为了这两个目标，将副本分布在不同的机器是不够的，它只防止了磁盘或机器的失效，以及最大化了机器的带宽利用率。我们也必须将副本分布到不同的机架上，这样可以确保在整个机架损坏或掉线（例如，由于网络交换或电源线路的问题造成的资源共享失效）的情况下，一个块的一些副本能够保存下来并能正常使用。这意味着在网络流量上，特别是在读取一个块时，可以利用多个机架的整合带宽。另一方面，写操作必须将数据导向多个机架，这个代价是我们愿意付出的。
>
> **创建、重新复制和重新负载均衡**
>
> 三个原因造成了块的复制：块创建、重新复制和重新负载均衡。
>
> 当主节点创建一个块时，它会选择在哪初始化一个空的副本。主节点会考虑几个因素：（1）我们想要在一个空间使用率低于平均值的块服务器上放置新的副本。经过一段时间，这将会使块服务器间的磁盘利用率基本相等。（2）我们想要限制每台块服务器上“最近”创建块的数量。尽管创建本身是廉价的，但是它也预示着马上就会有大量的数据写入，因为只有在需要进行写数据时，块才会被创建，在我们“一次写多次读”的工作中，他们通常一旦写完数据就变为只读的了。（3）如上面讨论的那样，我们想要将一个块的副本分布到不同的机架上。
>
> 当副本的可用数量低于用户指定的值时，Master会尽快进行块的重新复制。每个需要进行重新复制的块基于几个因素优先进行：一个是块现有的副本数量与目标数差多少。比如，一个丢失两个副本的块比丢失一个副本的优先级高；此外，相对于最近被删除的文件的块来说，我们优先对活跃的文件的块进行重新复制（4.4）；最后，为了最小化失效对正在运行的应用的影响，我们会提高阻塞客户进程的块的优先级。
>
> Master选取优先级最高的块，通过通知一些块服务器从一个存在的可用副本上拷贝块数据来“克隆”它。选择副本位置的方法与创建时类似：平均磁盘空间是使用率，限制单一块服务器上运行的克隆操作熟练，以及跨机架分布。为了防止克隆操作的流量影响到客户端的流量，Master限制了集群和每个块服务器上正在运行的克隆操作数量。此外，每个块服务器通过减少发往源块服务器的读请求，限制了每个克隆操作所占用的带宽。
>
> 最后，Master周期性的重新均衡副本的负载：它检查当前的副本分布情况，并将副本移动到更好的磁盘空间上，达到负载均衡。通过这个过程，Master逐步的填充新的块服务器，而不是马上使用新的块和大量的写入流量填充它。这个新副本的分布标准与上面提到的类似。此外，Master必须选择移动哪个已存在的副本，通常情况下，它更倾向于从剩余空间小于平均值的那个块服务器上移动副本，以使磁盘使用率相等。
>
> **垃圾回收**
>
> 在一个文件被删除后，GFS不会立即回收可用的物理存储空间。它只有在文件和块级别上定期的垃圾回收时才会进行。
>
> 当一个文件被应用删除时，Master会像其他操作一样立即记录下这个删除操作。然而，文件仅仅会被重命名为一个包含删除时间戳的、隐藏的名字，而不是立即回收资源。在Master定期的扫描文件系统的命名空间期间，它将真正删除已经被隐藏超过三天的文件（这个间隔可以配置），在此之前，文件仍然可以通过新的特殊的名字进行读取，也可以通过重命名为普通的文件名而撤销删除操作。当隐藏文件从命名空间中真正被删除时，内存中对应的元数据也会被清除，这样能有效的切断文件和它的所有块的连接。
>
> 在对块的命名空间做类似的扫描时，Master标识出孤儿块（任何文件都无法访问到的块），并将那些块的元数据清除。在与Master进行定期的心跳信息交换时，每个块服务器报告其所含有块的集合，Master回复块服务器哪些块没有出现在Master的元数据中，块服务器将释放并删除这些块的副本。
>
> 用于存储回收的垃圾回收方法通过惰性删除拥有几个优势。首先，对于组件失效是常态的大规模分布式系统来说，这种方式简单可靠。块创建操作可能在一些块服务器上执行成功，但在另一些服务器上执行失败，残余的副本在主节点上无法识别。副本删除消息可能丢失，Master必须重新发送执行失败的消息，包括自身的和块服务器的。垃圾回收机制提供了一个一致的、可靠的方法来清除没用的副本。其次，它将存储回收操作合并到了Master定期的后台操作中，比如，定期的浏览命名空间和与块服务器握手。因此，它被批处理完成，开销被分摊了。此外，只有在Master相对空闲时才会完成垃圾回收，Master对客户端需要及时回复的请求有更高的优先级进行响应。第三，延迟回收存储空间可以为防止意外的、不可撤销的删除操作提供一个安全保障。
>
> 主要的缺点是，当存储空间紧张时，会阻碍用户对使用的调优工作。频繁创建和删除临时文件的应用不能马上重用删除数据后的可用空间。我们通过在已被删除的文件上显式的再次进行删除来解决垃圾回收的这些问题。我们也可以允许用户对命名空间的不同部分应用不同的复制和回收策略。
>
> **过期副本的检测**
>
> 如果一个块服务器失效并在宕机期间丢失了块的修改操作，块副本可能会过期。对于每个块，Master维护一个块版本号来区分最新的副本和过期的副本。每当Master授予某个块一个新租约，它都会增加块的版本号，并通知最新的副本。Master和其它（Master的）副本都在它们的持久状态中记录了新的版本号，这会在任何客户端接收到通知并开始进行写操作之前进行。如果其它副本当前不可用，它的块版本号将不会被更新。当块服务器重启并报告它所有的块集合和它们相应的版本号时，Master还会探测到这个块服务器有过期的副本。如果Master发现有一个版本号高于自己的记录，Master会假设在它授予租约时失败，因此会选择更高的版本号作为最新的版本号。
>
> Master在定期的垃圾回收操作中清除过期的副本。在这之前，当它回复客户端的块信息请求时，它将认为一个过期的副本实际上根本并不存在。作为另一个安全措施，当Master通知客户端哪个块服务器持有这个块的租约时，或者在进行克隆操作期间它通知一个块服务器从另一个块服务器上读取数据时，包含块版本号。客户端或块服务器在执行操作时会验证这个版本号，以保证总是可访问的最新的数据。
>
> **容错和诊断**
>
> **高可用性: 快速恢复和复制。**
>
> Master和块服务器都被设计为无论它们如何终止都能够在几秒内恢复自己的状态并重新启动。实际上，我们并不区分正常的和不正常的终止
>
> 每个块被复制到不同机架上的多个块服务器上。用户能够为文件命名空间的不同部分指定不同的复制级别，默认为三个。当有块服务器掉线或通过校验和（5.2节）检测出损坏的副本时，Master克隆已存在的副本来保证每个块由足够的副本。尽管复制策略非常有效，但我们也在探索其他的跨服务器的冗余解决方案，如奇偶校验，或者纠删码（erasure code），来应对我们日益增长的只读存储需求。我们希望在我们的高度松耦合的系统中，这些复杂的冗余方案是具有挑战的，但并不是不可实现的，因为我们的操作主要是追加和读取，而不是小规模的随机写。
>
> 为了可靠性，Master的状态也被复制。它的操作日志和检查点被复制到多个机器上。一个修改操作只有在它的日志被刷新到本地磁盘和所有的Master副本后才认为被提交完成。简单的说，一个Master负责所有的操作，包括后台的行为，如垃圾回收等改变系统内部状态的行为。当它失效时，能够瞬间重启。如果它的机器或磁盘发送故障，GFS的外部监控会其它有副本操作日志的机器上启动一个新的Master。客户端只使用主节点的名字（如，gfs-test），它是一个DNS的别名，可以更改这个名字来访问被重新放置到其它机器上的Master。
>
> 此外，当primary Master宕机时，“影子”Master可以为文件系统提供一个只读的访问接口。它们是影子，而不是镜像，它们内部的数据会稍微的落后于primary Master，通常不到1秒。因此，它们对于那些不经常修改的文件的读取，或是不太在意得到的是稍微过期的数据的应用来说是有效的。实际中，因为文件内容是从块服务器读取的，所以应用察觉不到过期的文件内容。在短暂的时间窗口内，过期的可能是文件的元数据，如目录内容，或访问控制信息。
>
> 为了保持最新的状态，影子Master读取一个正在增长的操作日志副本，并执行与primary Master相同的操作序列来改变自己的数据结构。像primary那样，它在启动后轮询块服务器（之后很少）来定位块副本，通过频繁的交换握手信息来监控它们的状态。当primary Master决定创建或删除副本造成位置信息更新时，影子Master才会通过primary Master更新状态。
>
> **数据完整性**
>
> 每个块服务器使用校验和来探测存储的数据是否被损坏。我们能够使用块的其它副本来进行恢复，但是通过不同块服务器上的副本的比较无法探测出数据块是否损坏。此外，不同的副本可能是合理的：GFS修改操作的语法，特别是之前讨论过的原子追加操作，不能保证所有副本都相同。因此，每个块服务器必须独立的通过校验和验证它所拥有的拷贝是否合法。
>
> 一个块被分割成64KB大小的block，每个block都有一个对应的32bit校验和。像其它元数据一样，校验和保存在内存中，并通过日志永久存储，与用户数据分开。
>
> 对于读操作，在把数据返回给请求者之前，块服务器要验证读范围内所有数据block的校验和，因此，块服务器不会将损坏的数据传播到其它机器上。如果一个block与记录的校验和不匹配，块服务器会返回给请求者一个错误，并向Master报告这个错误。在响应中，请求者将从其它的副本读取数据，同时，Master将从其它副本上克隆这个块。在一个新的可用的副本复制完毕后，Master会通知报告错误的块服务器删除出错的副本。校验和对读操作几乎没有影响，原因有几个。因为大多数的读操作至少需要读取几个块，我们只需要读取一小部分额外的数据用于验证验证和。GFS客户端代码通过试图将读操作对齐到校验和的block边界上，大大减小了验证的开销。此外，块服务器上的校验和查询和对比不需要任何I/O就能完成，校验和的计算可以和I/O操作同时进行。
>
> 校验和计算针对在块尾部进行追加写操作做了很大的优化（相对于覆盖已有数据的写操作），因为写操作在我们的工作中是占主导的。我们仅增量更新最后一个不完整的块的校验和，并用追加的新的校验和来计算block新的校验和。即使最后一个不完整的校验和已经被损坏，并且我们现在没有探测出来，新的校验和将不匹配存储的数据，当这个块下一次被读取时，就是检测出数据已经损坏。
>
> 相反的，如果写操作覆盖块中已经存在的一个范围内，我们必须读取并验证要被覆盖的这个范围内的第一个和最后一个block，然后执行写操作，最后计算并记录新的校验和。如果我们在覆盖它们之前不进行对第一个和最后一个block的验证，新的校验和可能会隐藏没有被覆盖区域的错误。
>
> 在空闲的时候，块服务器能浏览和验证不活动跃的块的内容，这允许我们探测很少被读取的块的数据损坏。一旦损坏被探测到，Master就能创建一个新的没有损坏的副本，并删除已损坏的副本。这能够防止不活跃的、已损坏的块欺骗Master，使Master认为它是块的一个可用的副本。
>
> 

